# Future Plan (Ideas / Experiments)

This file tracks forward-looking ideas that are not implemented yet, but are likely follow-ups to the current Rust ⇄ TypeScript parity work.

## 3D NeuralNetwork Viewer + Cross-Runtime Mapping (Bevy ↔ WebGPU / three.js)

### Goal
Build a simple 3D viewer for the `Examples/NeuralNetwork` data model (a 3D grid: layers × nodes × weights) and explore a portability layer where the same conceptual scene could be driven by:

- Rust + Bevy (native)
- TypeScript + WebGPU (browser/desktop)
- TypeScript + three.js (fallback / rapid iteration)

The intention is to eventually have a shared “scene description” / rendering-adapter approach so the same visualization can be produced across runtimes.

### Why
- The NeuralNetwork example is already structured as a 3D grid and has seeded RNG parity across Rust and TS.
- A viewer helps debugging and communicating parity: it’s easier to spot mismatches visually than via raw numbers.
- A cross-runtime renderer mapping aligns with the repo’s broader theme of dual-binding and behavior parity.

### Candidate Approaches
- **Shared scene/geometry description** (recommended)
  - Define an intermediate representation (IR) for a scene: camera, transforms, meshes/lines/points, materials, labels.
  - Implement adapters:
    - IR → Bevy entities/components
    - IR → WebGPU draw calls (or a tiny wrapper library)
    - IR → three.js scene graph

- **Data-first, renderer-specific** (fastest MVP)
  - Implement three.js (or WebGPU) visualization first.
  - Add a Bevy implementation later by mirroring the layout math.

### MVP Scope
- Render nodes as points (or small cubes) in a 3D lattice.
- Render edges/weights as line segments or thin cylinders between layers.
- Color by weight magnitude/sign.
- Basic camera controls (orbit + zoom).
- Load data from:
  - Rust: directly from `Examples/NeuralNetwork` via a small viewer bin/tool.
  - TS: from the converted TS output or a JSON dump generated by Rust.

### Potential Milestones
1. Add a new tool under `tools/nn-viewer-web/` using three.js (quick iteration) or WebGPU (future-proof).
2. Define a small `nn_scene.json` schema (positions + weights + metadata) and emit it from Rust.
3. Implement a Bevy viewer under `tools/nn-viewer-bevy/` that consumes the same JSON schema.
4. Optionally evolve JSON schema into a proper scene IR and add adapters.

### Notes / Constraints
- Keep this separate from the core converter until the direction is proven.
- Prefer minimal dependencies initially.
- Reuse existing parity conventions when possible (seeded RNG, deterministic layout).
